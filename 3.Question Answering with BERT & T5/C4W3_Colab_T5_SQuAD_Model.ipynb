{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of C4W3_Colab_T5_SQuAD_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenzhenghe7/NLP-with-Attention-Models/blob/master/3.Question%20Answering%20with%20BERT%20%26%20T5/C4W3_Colab_T5_SQuAD_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cuAJ1dIrYNd",
        "outputId": "33dbcd84-09ea-4bea-9461-83c6e1b00f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 30 06:23:14 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    22W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yuytuIllsv1"
      },
      "source": [
        "# Assignment 3, Part 2: T5 SQuAD Model \n",
        "\n",
        "Welcome to the part 2 of testing the models for this week's assignment. This time we will perform decoding using the T5 SQuAD model. In this notebook we'll perform Question Answering by providing a \"Question\", its \"Context\" and see how well we get the \"Target\" answer. \n",
        "\n",
        "## IMPORTANT\n",
        "\n",
        "- As you cannot save the changes you make to this colab, you have to make a copy of this notebook in your own drive and run that. You can do so by going to `File -> Save a copy in Drive`. Close this colab and open the copy which you have made in your own drive.\n",
        "\n",
        "- Go to this [google drive folder](https://drive.google.com/drive/folders/1rOZsbEzcpMRVvgrRULRh1JPFpkIG_JOz?usp=sharing) named `NLP C4 W3 Colabs & Data`. In the folder, next to its name use the drop down menu to select `\"Add shortcut to Drive\" -> \"My Drive\" and then press ADD SHORTCUT`. This should add a shortcut to the folder `NLP C4 W3 Colabs & Data` within your own google drive. Please make sure this happens, as you'll be reading the data for this notebook from this folder.\n",
        "\n",
        "- Make sure your runtime is GPU (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6LQW5cMSgx"
      },
      "source": [
        "**Note: Restarting the runtime maybe required**.\n",
        "\n",
        "Colab will tell you if the restarting is necessary -- you can do this from the:\n",
        "\n",
        "Runtime > Restart Runtime\n",
        "\n",
        "option in the dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTeAcYQo_4OW"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- [Part 0: Downloading and loading dependencies](#0)\n",
        "- [Part 1: Mounting your drive for data accessibility](#1)\n",
        "- [Part 2: Getting things ready](#2)\n",
        "- [Part 3: Fine-tuning on SQuAD](#3)\n",
        "    - [3.1 Loading in the data and preprocessing](#3.1)\n",
        "    - [3.2 Decoding from a fine-tuned model](#3.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxogfC1M158"
      },
      "source": [
        "<a name='0'></a>\n",
        "# Part 0: Downloading and loading dependencies\n",
        "\n",
        "Uncomment the code cell below and run it to download some dependencies that you will need. You need to download them once every time you open the colab. You can ignore the `kfac` error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BNZzCg0xv3R",
        "outputId": "b4fa0b64-09b4-4f15-81c1-afe1d0b8a5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip -q install trax==1.3.4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 368kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 20.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 51.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 62.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 72.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 307kB 49.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 56.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 57.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 61.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 71.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5MB 56.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 70.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 48.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 52.5MB/s \n",
            "\u001b[?25h  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDhi6qLQMHzs",
        "outputId": "bcc1b962-1016-4790-dd89-58cb8d17ab6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import string\n",
        "import t5\n",
        "import numpy as np\n",
        "import trax \n",
        "from trax.supervised import decoding\n",
        "import textwrap \n",
        "# Will come handy later.\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwr7LoXwQUW5"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part 1: Mounting your drive for data accessibility\n",
        "\n",
        "Run the code cell below and follow the instructions to mount your drive. The data is the same as used in the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ZF7KiXzQEg",
        "outputId": "d6a1774e-ea4f-4e71-83ee-3fd4e9df3833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuTyft5EBQK6"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Part 2: Getting things ready \n",
        "\n",
        "Run the code cell below to ready some functions which will later help us in decoding. The code and the functions are the same as the ones you previsouly ran on the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8s_xZ1QtkI"
      },
      "source": [
        "PAD, EOS, UNK = 0, 1, 2\n",
        "\n",
        "def detokenize(np_array):\n",
        "  return trax.data.detokenize(\n",
        "      np_array,\n",
        "      vocab_type='sentencepiece',\n",
        "      vocab_file='sentencepiece.model',\n",
        "      vocab_dir='/content/drive/My Drive/NLP C4 W3 Colabs & Data/')\n",
        "\n",
        "def tokenize(s):\n",
        "  # The trax.data.tokenize function operates on streams,\n",
        "  # that's why we have to create 1-element stream with iter\n",
        "  # and later retrieve the result with next.\n",
        "  return next(trax.data.tokenize(\n",
        "      iter([s]),\n",
        "      vocab_type='sentencepiece',\n",
        "      vocab_file='sentencepiece.model',\n",
        "      vocab_dir='/content/drive/My Drive/NLP C4 W3 Colabs & Data/'))\n",
        " \n",
        "vocab_size = trax.data.vocab_size(\n",
        "    vocab_type='sentencepiece',\n",
        "    vocab_file='sentencepiece.model',\n",
        "    vocab_dir='/content/drive/My Drive/NLP C4 W3 Colabs & Data/')\n",
        "\n",
        "def get_sentinels(vocab_size):\n",
        "    sentinels = {}\n",
        "\n",
        "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
        "\n",
        "        decoded_text = detokenize([vocab_size - i]) \n",
        "        \n",
        "        # Sentinels, ex: <Z> - <a>\n",
        "        sentinels[decoded_text] = f'<{char}>'\n",
        "        \n",
        "    return sentinels\n",
        "\n",
        "sentinels = get_sentinels(vocab_size)    \n",
        "\n",
        "\n",
        "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
        "    # If already a string, just do the replacements.\n",
        "    if isinstance(encoded_str_list, (str, bytes)):\n",
        "        for token, char in sentinels.items():\n",
        "            encoded_str_list = encoded_str_list.replace(token, char)\n",
        "        return encoded_str_list\n",
        "  \n",
        "    # We need to decode and then prettyfy it.\n",
        "    return pretty_decode(detokenize(encoded_str_list))    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEoSSKNwgDVA"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part 3: Fine-tuning on SQuAD\n",
        "\n",
        "Now let's try to fine tune on SQuAD and see what becomes of the model. For this, we need to write a function that will create and process the SQuAD `tf.data.Dataset`. Below is how T5 pre-processes SQuAD dataset as a text2text example. Before we jump in, we will have to first load in the data. \n",
        "\n",
        "<a name='3.1'></a>\n",
        "### 3.1 Loading in the data and preprocessing\n",
        "\n",
        "You first start by loading in the dataset. The text2text example for a SQuAD example looks like:\n",
        "\n",
        "```\n",
        "{\n",
        "  'inputs': 'question: <question> context: <article>',\n",
        "  'targets': '<answer_0>',\n",
        "}\n",
        "```\n",
        "\n",
        "The squad pre-processing function takes in the dataset and processes it using the sentencePiece vocabulary you have seen above. It generates the features from the vocab and encodes the string features. It takes on question, context, and answer, and returns \"question: Q context: C\" as input and \"A\" as target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcdR5Dh9UVEw"
      },
      "source": [
        "# Retrieve Question, C, A and return \"question: Q context: C\" as input and \"A\" as target.\n",
        "def squad_preprocess_fn(dataset, mode='train'):\n",
        "  return t5.data.preprocessors.squad(dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NCIejwexv4x",
        "outputId": "24e20c99-eb4b-44ba-c828-8aa9d6b4533a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# train generator, this takes about 1 minute\n",
        "train_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n",
        "  'squad/v1.1:2.0.0',\n",
        "  data_dir='/content/drive/My Drive/NLP C4 W3 Colabs & Data/data/',\n",
        "  bare_preprocess_fn=squad_preprocess_fn,\n",
        "  input_name='inputs',\n",
        "  target_name='targets'\n",
        ")\n",
        "train_generator = train_generator_fn()\n",
        "next(train_generator)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(b'question: What year did Einstein discover the theory of general relativity ? context: The beginning of the 20th century brought the start of a revolution in physics . The long - held theories of Newton were shown not to be correct in all circumstances . Beginning in 1900 , Max Planck , Albert Einstein , Niels Bohr and others developed quantum theories to explain various anomalous experimental results , by introducing discrete energy levels . Not only did quantum mechanics show that the laws of motion did not hold on small scales , but even more disturbingly , the theory of general relativity , proposed by Einstein in 1915 , showed that the fixed background of spacetime , on which both Newtonian mechanics and special relativity depended , could not exist . In 1925 , Werner Heisenberg and Erwin Schr \\xc3\\xb6 dinger formulated quantum mechanics , which explained the preceding quantum theories . The observation by Edwin Hubble in 1929 that the speed at which galaxies recede positively correlates with their distance , led to the understanding that the universe is expanding , and the formulation of the Big Bang theory by Georges Lema \\xc3\\xae tre . ',\n",
              " b'1915')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGQsExH8xv40",
        "outputId": "6d769e88-9eb0-4f4f-a716-18cb9fb73d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#print example from train_generator\n",
        "(inp, out) = next(train_generator)\n",
        "print(inp.decode('utf8').split('context:')[0])\n",
        "print()\n",
        "print('context:', inp.decode('utf8').split('context:')[1])\n",
        "print()\n",
        "print('target:', out.decode('utf8'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "question: What category of hurricane was Hazel ? \n",
            "\n",
            "context:  Severe weather occurs regularly in North Carolina . On the average , a hurricane hits the state once a decade . Destructive hurricanes that have struck the state include Hurricane Fran , Hurricane Floyd , and Hurricane Hazel , the strongest storm to make landfall in the state , as a Category 4 in 1954 . Hurricane Isabel stands out as the most damaging of the 21st century . Tropical storms arrive every 3 or 4 years . In addition , many hurricanes and tropical storms graze the state . In some years , several hurricanes or tropical storms can directly strike the state or brush across the coastal areas . Only Florida and Louisiana are hit by hurricanes more often . Although many people believe that hurricanes menace only coastal areas , the rare hurricane which moves inland quickly enough can cause severe damage ; for example , in 1989 , Hurricane Hugo caused heavy damage in Charlotte and even as far inland as the Blue Ridge Mountains in the northwestern part of the state . On the average , North Carolina has 50 days of thunderstorm activity per year , with some storms becoming severe enough to produce hail , flash floods , and damaging winds . \n",
            "\n",
            "target: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_eGJHpPh-rz"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 Decoding from a fine-tuned model\n",
        "\n",
        "You will now use an existing model that we trained for you. You will initialize, then load in your model, and then try with your own input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZztX5XLa01p"
      },
      "source": [
        "# Initialize the model \n",
        "model = trax.models.Transformer(\n",
        "    d_ff = 4096,\n",
        "    d_model = 1024,\n",
        "    max_len = 2048,\n",
        "    n_heads = 16,\n",
        "    dropout = 0.1,\n",
        "    input_vocab_size = 32000,\n",
        "    n_encoder_layers = 24,\n",
        "    n_decoder_layers = 24,\n",
        "    mode='predict')  # Change to 'eval' for slow decoding."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRAGG2fnSMe"
      },
      "source": [
        "# load in the model\n",
        "# this will take a minute\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "model.init_from_file('/content/drive/My Drive/NLP C4 W3 Colabs & Data/models/model_squad.pkl.gz',\n",
        "                     weights_only=True, input_signature=(shape11, shape11))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l33umWrRyYNm"
      },
      "source": [
        "# Uncomment to see the transformer's structure.\n",
        "# print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzrxtGGDuUkc"
      },
      "source": [
        "# create inputs\n",
        "# a simple example \n",
        "# inputs = 'question: She asked him where is john? context: John was at the game'\n",
        "\n",
        "# an extensive example\n",
        "inputs = 'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVlu-iwHcZRY",
        "outputId": "f4038988-036a-497a-8d86-087891283f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# tokenizing the input so we could feed it for decoding\n",
        "print(tokenize(inputs))\n",
        "test_inputs = tokenize(inputs) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  822    10   363    33   128    13     8  6548    13     3     9  4659\n",
            "    58  2625    10    71  4659    19     3     9  1679    63 24999  5624\n",
            "    53  1475    13     8     3   729   302 15641     6    16     8   384\n",
            " 15641  8433    15     6    42     8  5624    34  4595     7     5  7238\n",
            "    33   147   386  6189  3244    11     3   324     7    13  2909    13\n",
            " 10357   291     7     5   328   607     3     9   563    13  2677    24\n",
            "    54    36     3    15 12621 21675     7     6 11908     6    42  5032\n",
            "    53     6    28  6269     7    24    33   557     3  8715    28  4816\n",
            "     3  2246 19376     7     5 20294  5215    16   812    11  2346    11\n",
            "    33  1086   508    11   504    63     6    16  6548     3  6836    45\n",
            "   872   190  4459     7    11  1131     7     5  1377  3244    33  4262\n",
            "    12  3826     6    28  2755  2302  4262    12  1740     6  1117  1371\n",
            "     6    11  3457 24411  2648     5     3  7727   725     6 10357   291\n",
            "     7    11  9279     7    33    66  5456  4503    21    70  2790    11\n",
            "   557    33 29346     5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJh_Qw9G5jND",
        "outputId": "fd2d0bbb-774b-4813-f3fc-dff3972c4666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Temperature is a parameter for sampling.\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n",
        "                                        temperature=0.0, max_length=10)\n",
        "print(wrapper.fill(pretty_decode(output[0])))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "white through yellows and reds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BadBty9C-8wY"
      },
      "source": [
        "### Note: As you can see the RAM is almost full, it is because the model and the decoding is memory heavy. You can run decoding just once. Running it the second time with another example might give you the same answer as before, or not run at all (crash). If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again."
      ]
    }
  ]
}